{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Math\nimport math\n\n# HuggingFace libraries \nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Pathlib \nfrom pathlib import Path\n\n# typing\nfrom typing import Any\n\n# Library for progress bars in loops\nfrom tqdm import tqdm\n\n# Importing library of warnings\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-27T06:19:46.395664Z","iopub.execute_input":"2025-09-27T06:19:46.396710Z","iopub.status.idle":"2025-09-27T06:20:11.919468Z","shell.execute_reply.started":"2025-09-27T06:19:46.396667Z","shell.execute_reply":"2025-09-27T06:20:11.918422Z"}},"outputs":[{"name":"stderr","text":"2025-09-27 06:19:54.580176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758953994.788609      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758953994.852052      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Architecture","metadata":{}},{"cell_type":"markdown","source":"![Image](https://shreyansh26.github.io/assets/img/posts_images/attention/arch.PNG)","metadata":{}},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"markdown","source":"![Tokenizer](https://i.ytimg.com/vi/hL4ZnAWSyuU/sddefault.jpg)","metadata":{}},{"cell_type":"code","source":"def build_tokenizer(config, ds, lang):\n    \n    # Crating a file path for the tokenizer \n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    \n    # Checking if Tokenizer already exists\n    if not Path.exists(tokenizer_path): \n        \n        # If it doesn't exist, we create a new one\n        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n        \n        # Creating a trainer for the new tokenizer\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \n                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n        \n        # Training new tokenizer on sentences from the dataset and language specified \n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n    return tokenizer # Returns the loaded tokenizer or the trained tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T09:28:37.352640Z","iopub.execute_input":"2025-09-26T09:28:37.353468Z","iopub.status.idle":"2025-09-26T09:28:37.363428Z","shell.execute_reply.started":"2025-09-26T09:28:37.353434Z","shell.execute_reply":"2025-09-26T09:28:37.362504Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_config():\n    return{\n        'batch_size': 8,\n        'num_epochs': 20,\n        'lr': 10**-4,\n        'seq_len': 350,\n        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n        'lang_src': 'en',\n        'lang_tgt': 'it',\n        'model_folder': 'weights',\n        'model_basename': 'translation_model_',\n        'preload': None,\n        'tokenizer_file': 'tokenizer_{0}.json',\n        'experiment_name': 'runs/translation_model'\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T05:46:33.856783Z","iopub.execute_input":"2025-09-27T05:46:33.857115Z","iopub.status.idle":"2025-09-27T05:46:33.866767Z","shell.execute_reply.started":"2025-09-27T05:46:33.857091Z","shell.execute_reply":"2025-09-27T05:46:33.865784Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for pair in ds:\n        yield pair['translation'][lang]\n\ndef get_ds(config):\n    \n    # Loading the train portion of the OpusBooks dataset.\n    # The Language pairs will be defined in the 'config' dictionary we will build later\n    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train') \n    \n    # Building or loading tokenizer for both the source and target languages \n    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n    \n    # Splitting the dataset for training and validation \n    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n                                    \n    # Processing data with the BilingualDataset class, which we will define below\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n                                    \n    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n    max_len_src = 0\n    max_len_tgt = 0\n    for pair in ds_raw:\n        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n        \n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n    \n    # Creating dataloaders for the training and validadion sets\n    # Dataloaders are used to iterate over the dataset in batches during training and validation\n    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n    \n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers\n\ndef casual_mask(size):\n        # Creating a square matrix of dimensions 'size x size' filled with ones\n        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n        return mask == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T09:29:23.171985Z","iopub.execute_input":"2025-09-26T09:29:23.172722Z","iopub.status.idle":"2025-09-26T09:29:23.185773Z","shell.execute_reply.started":"2025-09-26T09:29:23.172689Z","shell.execute_reply":"2025-09-26T09:29:23.184669Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n    \n    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n    # 'seq_len' defines the sequence length for both languages\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n        \n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        \n        # Defining special tokens by using the target language tokenizer\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n        \n    # Total number of instances in the dataset (some pairs are larger than others)\n    def __len__(self):\n        return len(self.ds)\n    \n    # Using the index to retrive source and target texts\n    def __getitem__(self, index: Any) -> Any:\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n        \n        # Tokenizing source and target texts \n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n        \n        # Computing how many padding tokens need to be added to the tokenized texts \n        # Source tokens\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n        # Target tokens\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n        \n        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n        # given the current sequence length limit (this will be defined in the config dictionary below)\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n         \n        # Building the encoder input tensor by combining several elements\n        encoder_input = torch.cat(\n            [\n            self.sos_token, # inserting the '[SOS]' token\n            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n            self.eos_token, # Inserting the '[EOS]' token\n            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n        )\n        \n        # Building the decoder input tensor by combining several elements\n        decoder_input = torch.cat(\n            [\n                self.sos_token, # inserting the '[SOS]' token \n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n        \n        )\n        \n        # Creating a label tensor, the expected output for training the model\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                self.eos_token, # Inserting the '[EOS]' token \n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n                \n            ]\n        )\n        \n        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n        \n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input, \n            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)), \n            'label': label,\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T09:29:25.224915Z","iopub.execute_input":"2025-09-26T09:29:25.225269Z","iopub.status.idle":"2025-09-26T09:29:25.240795Z","shell.execute_reply.started":"2025-09-26T09:29:25.225242Z","shell.execute_reply":"2025-09-26T09:29:25.239521Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Token Embedding","metadata":{}},{"cell_type":"code","source":"class TokenEmbeddings(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embeddings = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embeddings(x) * math.sqrt(self.d_model) # Normalizing the variance of the embedding\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Positional Encoding\n\n<p style=\"\n    margin-bottom: 5; \n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000; \n  \">\n    \\begin{equation}\n    \\text{Even Indices } (2i): \\quad \\text{PE(pos, } 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n    \\end{equation}\n</p>\n\n<p style=\"\n    margin-bottom: 5; \n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000; \n  \">\n    \\begin{equation}\n    \\text{Odd Indices } (2i + 1): \\quad \\text{PE(pos, } 2i + 1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n    \\end{equation}\n</p>","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, seq_len: int):\n        self.d_model = d_model\n        self.seq_len = seq_len\n        encodings = torch.zeros(seq_len, d_model)\n        pos = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # [seq_len, 1]\n        denominator = (torch.arange(0, d_model, 2).float() / d_model)\n        denominator = torch.pow(denominator, 10000)\n\n        encodings[:, 0::2] = torch.sin(pos/denominator)\n        encodings[:, 1::2] = torch.cos(pos/denominator)\n        encodings = encodings.unsqueeze(0) # add a batch dimension\n        self.register_buffer('encodings', encodings) # Buffer is a tensor not considered as a model parameter\n\n\n    def forward(self, x):\n        return x + (self.encodings[:, :x.shape[1], :]).requires_grad_(False)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T06:42:30.573266Z","iopub.execute_input":"2025-09-27T06:42:30.573621Z","iopub.status.idle":"2025-09-27T06:42:30.581002Z","shell.execute_reply.started":"2025-09-27T06:42:30.573597Z","shell.execute_reply":"2025-09-27T06:42:30.579776Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T06:20:43.358841Z","iopub.execute_input":"2025-09-27T06:20:43.359161Z","iopub.status.idle":"2025-09-27T06:20:43.373954Z","shell.execute_reply.started":"2025-09-27T06:20:43.359138Z","shell.execute_reply":"2025-09-27T06:20:43.372697Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([512, 1])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}