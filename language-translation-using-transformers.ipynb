{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Math\nimport math\n\n# HuggingFace libraries \nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Pathlib \nfrom pathlib import Path\n\n# typing\nfrom typing import Any\n\n# Library for progress bars in loops\nfrom tqdm import tqdm\n\n# Importing library of warnings\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:48:56.025194Z","iopub.execute_input":"2025-09-30T06:48:56.025837Z","iopub.status.idle":"2025-09-30T06:49:15.242234Z","shell.execute_reply.started":"2025-09-30T06:48:56.025805Z","shell.execute_reply":"2025-09-30T06:49:15.241619Z"}},"outputs":[{"name":"stderr","text":"2025-09-30 06:49:02.170098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759214942.349948      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759214942.403617      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Architecture","metadata":{}},{"cell_type":"markdown","source":"![Image](https://shreyansh26.github.io/assets/img/posts_images/attention/arch.PNG)","metadata":{}},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"markdown","source":"![Tokenizer](https://i.ytimg.com/vi/hL4ZnAWSyuU/sddefault.jpg)","metadata":{}},{"cell_type":"code","source":"def build_tokenizer(config, ds, lang):\n    \n    # Crating a file path for the tokenizer \n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    \n    # Checking if Tokenizer already exists\n    if not Path.exists(tokenizer_path): \n        \n        # If it doesn't exist, we create a new one\n        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n        \n        # Creating a trainer for the new tokenizer\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \n                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n        \n        # Training new tokenizer on sentences from the dataset and language specified \n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n    return tokenizer # Returns the loaded tokenizer or the trained tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.243358Z","iopub.execute_input":"2025-09-30T06:49:15.244037Z","iopub.status.idle":"2025-09-30T06:49:15.249330Z","shell.execute_reply.started":"2025-09-30T06:49:15.244017Z","shell.execute_reply":"2025-09-30T06:49:15.248594Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def get_config():\n    return{\n        'batch_size': 8,\n        'num_epochs': 20,\n        'lr': 10**-4,\n        'seq_len': 350,\n        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n        'lang_src': 'en',\n        'lang_tgt': 'it',\n        'model_folder': 'weights',\n        'model_basename': 'translation_model_',\n        'preload': None,\n        'tokenizer_file': 'tokenizer_{0}.json',\n        'experiment_name': 'runs/translation_model',\n        'encoder_layers': 6,\n        'decoder_layers': 6,\n        'p_drop': 0.1,\n        'dff': 2048,\n        'n_heads': 8\n    }\n\nconfig = get_config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.250037Z","iopub.execute_input":"2025-09-30T06:49:15.250268Z","iopub.status.idle":"2025-09-30T06:49:15.297772Z","shell.execute_reply.started":"2025-09-30T06:49:15.250251Z","shell.execute_reply":"2025-09-30T06:49:15.296837Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for pair in ds:\n        yield pair['translation'][lang]\n        \ntokenizer_src = None\ntokenizer_tgt = None\ntrain_ds = None\ndef get_ds(config):\n    \n    # Loading the train portion of the OpusBooks dataset.\n    # The Language pairs will be defined in the 'config' dictionary we will build later\n    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train') \n    \n    # Building or loading tokenizer for both the source and target languages \n    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n    \n    # Splitting the dataset for training and validation \n    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n                                    \n    # Processing data with the BilingualDataset class, which we will define below\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n                                    \n    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n    max_len_src = 0\n    max_len_tgt = 0\n    for pair in ds_raw:\n        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n        \n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n    \n    # Creating dataloaders for the training and validadion sets\n    # Dataloaders are used to iterate over the dataset in batches during training and validation\n    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n    \n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers\n\ndef casual_mask(size):\n        # Creating a square matrix of dimensions 'size x size' filled with ones\n        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n        return mask == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.299656Z","iopub.execute_input":"2025-09-30T06:49:15.299931Z","iopub.status.idle":"2025-09-30T06:49:15.314642Z","shell.execute_reply.started":"2025-09-30T06:49:15.299903Z","shell.execute_reply":"2025-09-30T06:49:15.314007Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n    \n    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n    # 'seq_len' defines the sequence length for both languages\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n        super().__init__()\n        \n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        \n        # Defining special tokens by using the target language tokenizer\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n        \n    # Total number of instances in the dataset (some pairs are larger than others)\n    def __len__(self):\n        return len(self.ds)\n    \n    # Using the index to retrive source and target texts\n    def __getitem__(self, index: Any) -> Any:\n        src_target_pair = self.ds[index]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n        \n        # Tokenizing source and target texts \n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n        \n        # Computing how many padding tokens need to be added to the tokenized texts \n        # Source tokens\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n        # Target tokens\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n        \n        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n        # given the current sequence length limit (this will be defined in the config dictionary below)\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError('Sentence is too long')\n         \n        # Building the encoder input tensor by combining several elements\n        encoder_input = torch.cat(\n            [\n            self.sos_token, # inserting the '[SOS]' token\n            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n            self.eos_token, # Inserting the '[EOS]' token\n            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n        )\n        \n        # Building the decoder input tensor by combining several elements\n        decoder_input = torch.cat(\n            [\n                self.sos_token, # inserting the '[SOS]' token \n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n            ]\n        \n        )\n        \n        # Creating a label tensor, the expected output for training the model\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n                self.eos_token, # Inserting the '[EOS]' token \n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n                \n            ]\n        )\n        \n        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n        \n        return {\n            'encoder_input': encoder_input,\n            'decoder_input': decoder_input, \n            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).int(),\n            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).int() & (causal_mask(decoder_input.size(0)).squeeze(0)), \n            'label': label,\n            'src_text': src_text,\n            'tgt_text': tgt_text\n        }    \n\ndef causal_mask(size):\n        # Creating a square matrix of dimensions 'size x size' filled with ones\n        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n        return mask == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.315480Z","iopub.execute_input":"2025-09-30T06:49:15.315763Z","iopub.status.idle":"2025-09-30T06:49:15.331666Z","shell.execute_reply.started":"2025-09-30T06:49:15.315745Z","shell.execute_reply":"2025-09-30T06:49:15.330922Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Token Embedding","metadata":{}},{"cell_type":"code","source":"class TokenEmbeddings(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embeddings = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embeddings(x) * math.sqrt(self.d_model) # Normalizing the variance of the embedding\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.332454Z","iopub.execute_input":"2025-09-30T06:49:15.333068Z","iopub.status.idle":"2025-09-30T06:49:15.344982Z","shell.execute_reply.started":"2025-09-30T06:49:15.333040Z","shell.execute_reply":"2025-09-30T06:49:15.344253Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Positional Encoding\n\n<p style=\"\n    margin-bottom: 5; \n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000; \n  \">\n    \\begin{equation}\n    \\text{Even Indices } (2i): \\quad \\text{PE(pos, } 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n    \\end{equation}\n</p>\n\n<p style=\"\n    margin-bottom: 5; \n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000; \n  \">\n    \\begin{equation}\n    \\text{Odd Indices } (2i + 1): \\quad \\text{PE(pos, } 2i + 1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n    \\end{equation}\n</p>","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, seq_len: int):\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        encodings = torch.zeros(seq_len, d_model)\n        pos = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # [seq_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        encodings[:, 0::2] = torch.sin(pos * div_term)\n        encodings[:, 1::2] = torch.cos(pos * div_term)\n        encodings = encodings.unsqueeze(0) # add a batch dimension\n        self.register_buffer('encodings', encodings) # Buffer is a tensor not considered as a model parameter\n\n\n    def forward(self, x):\n        return x + (self.encodings[:, :x.shape[1], :]).requires_grad_(False)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.345643Z","iopub.execute_input":"2025-09-30T06:49:15.345869Z","iopub.status.idle":"2025-09-30T06:49:15.359709Z","shell.execute_reply.started":"2025-09-30T06:49:15.345834Z","shell.execute_reply":"2025-09-30T06:49:15.359134Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"posm = PositionalEncoding(512, 350)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.360424Z","iopub.execute_input":"2025-09-30T06:49:15.360671Z","iopub.status.idle":"2025-09-30T06:49:15.442026Z","shell.execute_reply.started":"2025-09-30T06:49:15.360645Z","shell.execute_reply":"2025-09-30T06:49:15.441451Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Layer Norm","metadata":{}},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, eps: float = 1e-9):\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(1))\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.alpha * (x-mean) / (std+self.eps) + self.bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.442710Z","iopub.execute_input":"2025-09-30T06:49:15.442971Z","iopub.status.idle":"2025-09-30T06:49:15.447441Z","shell.execute_reply.started":"2025-09-30T06:49:15.442950Z","shell.execute_reply":"2025-09-30T06:49:15.446833Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# FFW","metadata":{}},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):\n    \n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        # First linear transformation\n        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n        # Second linear transformation\n        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n        \n    def forward(self, x):\n        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.449666Z","iopub.execute_input":"2025-09-30T06:49:15.449928Z","iopub.status.idle":"2025-09-30T06:49:15.458332Z","shell.execute_reply.started":"2025-09-30T06:49:15.449911Z","shell.execute_reply":"2025-09-30T06:49:15.457670Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Multi Head Attention\n\n<center>\n    <img src = \"https://i.imgur.com/JqJVrsj.png\" width = 1556, height= 959>\n<p style = \"font-size: 16px;\n            font-family: 'Georgia', serif;\n            text-align: center;\n            margin-top: 10px;\"></p>\n</center>","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, n_heads: int, d_model: int):\n        super().__init__()\n        self.n_heads = n_heads\n        self.w_key = nn.Linear(d_model, d_model)\n        self.w_query = nn.Linear(d_model, d_model)\n        self.w_value = nn.Linear(d_model, d_model)\n        self.w_out = nn.Linear(d_model, d_model)\n\n    def attention(self, k, q, v, mask):\n        d_k = q.shape[-1]\n        affinities = (q @ k.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            affinities.masked_fill_(mask == 0, -1e9)\n        affinities = affinities.softmax(dim=-1)\n        value = affinities @ v\n        return value\n        \n\n    def forward(self, q, k, v, mask):\n        key = self.w_key(k)\n        query = self.w_query(q)\n        value = self.w_value(v)\n\n        # split embedding dim for each heads\n        new_d_model = config['d_model'] // self.n_heads\n        k_chunks = torch.split(key, new_d_model, dim=-1)\n        q_chunks = torch.split(query, new_d_model, dim=-1)\n        v_chunks = torch.split(value, new_d_model, dim=-1)\n\n        output_heads = []\n        for i in range(self.n_heads):\n            output_heads.append(self.attention(k_chunks[i], q_chunks[i], v_chunks[i], mask))\n\n        concat_out = torch.cat(output_heads, dim=-1)\n        return self.w_out(concat_out)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.459049Z","iopub.execute_input":"2025-09-30T06:49:15.459640Z","iopub.status.idle":"2025-09-30T06:49:15.471603Z","shell.execute_reply.started":"2025-09-30T06:49:15.459623Z","shell.execute_reply":"2025-09-30T06:49:15.471112Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layernorm = LayerNorm()\n\n    def forward(self, x, sub_layer):\n        return x + sub_layer(self.layernorm(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.472309Z","iopub.execute_input":"2025-09-30T06:49:15.472976Z","iopub.status.idle":"2025-09-30T06:49:15.483527Z","shell.execute_reply.started":"2025-09-30T06:49:15.472957Z","shell.execute_reply":"2025-09-30T06:49:15.483009Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Encoder\n<center>\n    <img src = \"https://www.researchgate.net/profile/Ehsan-Amjadian/publication/352239001/figure/fig1/AS:1033334390013952@1623377525434/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an.jpg\" width = 400, height= 400>\n<p style = \"font-size: 16px;\n            font-family: 'Georgia', serif;\n            text-align: center;\n            margin-top: 10px;\">Encoder block. Source: <a href = \"https:///figure/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an_fig1_352239001\">researchgate.net</a>.</p>\n</center>","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet_mha = ResidualConnection()\n        self.resnet_ffw = ResidualConnection()\n        self.mha = MultiHeadAttention(n_heads=config['n_heads'], d_model=config['d_model'])\n        self.ffw = FeedForwardBlock(d_model=config['d_model'], d_ff=config['dff'], dropout=config['p_drop'])\n\n    def forward(self, x, src_mask):\n        x = self.resnet_mha(x, lambda x : self.mha(x, x, x, src_mask))\n        x = self.resnet_ffw(x, lambda x : self.ffw(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.484101Z","iopub.execute_input":"2025-09-30T06:49:15.484295Z","iopub.status.idle":"2025-09-30T06:49:15.498100Z","shell.execute_reply.started":"2025-09-30T06:49:15.484273Z","shell.execute_reply":"2025-09-30T06:49:15.497487Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder_blocks = nn.ModuleList(\n            [EncoderBlock() for _ in range(config['encoder_layers'])]\n        )\n        \n    def forward(self, x, src_mask):\n        for block in self.encoder_blocks:\n            x = block(x, src_mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.498944Z","iopub.execute_input":"2025-09-30T06:49:15.499173Z","iopub.status.idle":"2025-09-30T06:49:15.509580Z","shell.execute_reply.started":"2025-09-30T06:49:15.499153Z","shell.execute_reply":"2025-09-30T06:49:15.508922Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Decoder\n\n<center>\n    <img src = \"https://res.cloudinary.com/edlitera/image/upload/c_fill,f_auto/v1680629118/blog/gz5ccspg3yvq4eo6xhrr\" width = 400, height= 400>\n<p style = \"font-size: 16px;\n            font-family: 'Georgia', serif;\n            text-align: center;\n            margin-top: 10px;\"></p>\n</center>","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet_blocks = nn.ModuleList([ResidualConnection() for _ in range(3)])\n        self.self_mha = MultiHeadAttention(n_heads=config['n_heads'], d_model=config['d_model'])\n        self.cross_mha = MultiHeadAttention(n_heads=config['n_heads'], d_model=config['d_model'])\n        self.ffw = FeedForwardBlock(d_model=config['d_model'], d_ff=config['dff'], dropout=config['p_drop'])\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.resnet_blocks[0](x, lambda x: self.self_mha(x, x, x, tgt_mask))\n        x = self.resnet_blocks[1](x, lambda x: self.cross_mha(x, encoder_output, encoder_output, src_mask))\n        x = self.resnet_blocks[2](x, lambda x: self.ffw(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.510386Z","iopub.execute_input":"2025-09-30T06:49:15.511036Z","iopub.status.idle":"2025-09-30T06:49:15.520922Z","shell.execute_reply.started":"2025-09-30T06:49:15.511012Z","shell.execute_reply":"2025-09-30T06:49:15.520348Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.decoder_block = nn.ModuleList(\n            [DecoderBlock() for _ in range(config['decoder_layers'])]\n        )\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.decoder_block:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n\n        return x ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.521597Z","iopub.execute_input":"2025-09-30T06:49:15.521756Z","iopub.status.idle":"2025-09-30T06:49:15.532720Z","shell.execute_reply.started":"2025-09-30T06:49:15.521743Z","shell.execute_reply":"2025-09-30T06:49:15.532061Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"code","source":"class TranslationTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder_embeddings = TokenEmbeddings(config['d_model'], tokenizer_src.get_vocab_size())\n        self.encoder = Encoder()\n        self.decoder_embeddings = TokenEmbeddings(config['d_model'], tokenizer_tgt.get_vocab_size())\n        self.decoder = Decoder()\n        self.positional_encodings = PositionalEncoding(config['d_model'], config['seq_len'])\n        self.projection = nn.Linear(config['d_model'], tokenizer_tgt.get_vocab_size())\n\n    def encode(self, encoder_inp, src_mask):\n        encoder_embeddings = self.encoder_embeddings(encoder_inp)\n        encoder_embeddings = self.positional_encodings(encoder_embeddings)\n        encoder_output = self.encoder(encoder_embeddings, src_mask)\n        return encoder_output\n\n    def decode(self, encoder_output, decoder_inp, src_mask, tgt_mask):\n        decoder_embeddings = self.decoder_embeddings(decoder_inp)\n        decoder_embeddings = self.positional_encodings(decoder_embeddings)\n        decoder_output = self.decoder(decoder_embeddings, encoder_output, src_mask, tgt_mask)\n        output = torch.log_softmax(self.projection(decoder_output), dim = -1)\n        return output\n\n    def generate(self, encoder_inp, src_mask):\n        encoder_output = self.encode(encoder_inp, src_mask)\n\n        sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n        eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n        decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_inp).to(device)\n\n        while decoder_input.shape[1] < config['seq_len']:\n            decoder_output = self.decode(encoder_output, decoder_input, src_mask, causal_mask(decoder_input.shape[1]).type_as(src_mask))\n            output_token = torch.argmax(decoder_output[0, -1, :]) #greedy sampling\n            decoder_input = torch.cat([decoder_input, torch.empty(1,1).fill_(output_token).type_as(encoder_inp).to(device)], dim=1)\n\n            if output_token == eos_idx:\n                break\n\n        return decoder_input.squeeze(0)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.533323Z","iopub.execute_input":"2025-09-30T06:49:15.533544Z","iopub.status.idle":"2025-09-30T06:49:15.549793Z","shell.execute_reply.started":"2025-09-30T06:49:15.533521Z","shell.execute_reply":"2025-09-30T06:49:15.549303Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"a = torch.randn(3,4,5)\nprint(a[0, -1, :])\ntorch.argmax(a[0, -1, :])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.550506Z","iopub.execute_input":"2025-09-30T06:49:15.550714Z","iopub.status.idle":"2025-09-30T06:49:15.599686Z","shell.execute_reply.started":"2025-09-30T06:49:15.550690Z","shell.execute_reply":"2025-09-30T06:49:15.599040Z"}},"outputs":[{"name":"stdout","text":"tensor([ 1.5589, -2.1769,  0.2512,  0.4466, -0.1714])\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor(0)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, writer, num_examples=2):\n    model.eval() # Setting model to evaluation mode\n    count = 0 # Initializing counter to keep track of how many examples have been processed\n    \n    console_width = 80 # Fixed witdh for printed messages\n    \n    # Creating evaluation loop\n    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch['encoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            \n            # Ensuring that the batch_size of the validation set is 1\n            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n            \n            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n            model_out = model.generate(encoder_input, encoder_mask)\n            \n            # Retrieving source and target texts from the batch\n            source_text = batch['src_text'][0]\n            target_text = batch['tgt_text'][0] # True translation \n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n            \n            # Printing results\n            print_msg('-'*console_width)\n            print_msg(f'SOURCE: {source_text}')\n            print_msg(f'TARGET: {target_text}')\n            print_msg(f'PREDICTED: {model_out_text}')\n            \n            # After two examples, we break the loop\n            if count == num_examples:\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.600409Z","iopub.execute_input":"2025-09-30T06:49:15.600775Z","iopub.status.idle":"2025-09-30T06:49:15.605914Z","shell.execute_reply.started":"2025-09-30T06:49:15.600757Z","shell.execute_reply":"2025-09-30T06:49:15.605312Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:15.606642Z","iopub.execute_input":"2025-09-30T06:49:15.606825Z","iopub.status.idle":"2025-09-30T06:49:25.261942Z","shell.execute_reply.started":"2025-09-30T06:49:15.606811Z","shell.execute_reply":"2025-09-30T06:49:25.261153Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f5f2d9eb296483a90b4ddc5a924c001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"en-it/train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd91d0ad09b4ec981b1da818272b66c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"004704f5efb146f488dbfe1eb02533fa"}},"metadata":{}},{"name":"stdout","text":"Max length of source sentence: 309\nMax length of target sentence: 274\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"data = train_dataloader.dataset.__getitem__(100)\nprint(data['decoder_mask'].shape)\nprint(data['encoder_mask'].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:25.262751Z","iopub.execute_input":"2025-09-30T06:49:25.263016Z","iopub.status.idle":"2025-09-30T06:49:25.274702Z","shell.execute_reply.started":"2025-09-30T06:49:25.262989Z","shell.execute_reply":"2025-09-30T06:49:25.274146Z"}},"outputs":[{"name":"stdout","text":"torch.Size([350, 350])\ntorch.Size([1, 350])\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"model = TranslationTransformer().to(device)\n    \n# Initialize the parameters\nfor p in model.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:25.275443Z","iopub.execute_input":"2025-09-30T06:49:25.275700Z","iopub.status.idle":"2025-09-30T06:49:26.240666Z","shell.execute_reply.started":"2025-09-30T06:49:25.275678Z","shell.execute_reply":"2025-09-30T06:49:26.239892Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train_model():\n    writer = SummaryWriter(config['experiment_name'])\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n\n    for epoch in range(config['num_epochs']):\n        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n        for batch in batch_iterator:\n            model.train()\n            encoder_input = batch['encoder_input'].to(device)\n            decoder_input = batch['decoder_input'].to(device)\n            encoder_mask = batch['encoder_mask'].to(device)\n            decoder_mask = batch['decoder_mask'].to(device)\n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, decoder_input, encoder_mask, decoder_mask)\n\n            label = batch['label'].to(device)\n            \n            loss = loss_fn(decoder_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            \n            # Updating progress bar\n            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n            \n            writer.add_scalar('train loss', loss.item())\n            writer.flush()\n            \n            # Performing backpropagation\n            loss.backward()\n            \n            # Updating parameters based on the gradients\n            optimizer.step()\n            \n            # Clearing the gradients to prepare for the next batch\n            optimizer.zero_grad()\n\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), writer)\n         \n        # Writting current model state to the 'model_filename'\n        torch.save({\n            'epoch': epoch, # Current epoch\n            'model_state_dict': model.state_dict(),# Current model state\n            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n        }, f'checkpoint_{epoch}.pth')    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T06:49:58.304925Z","iopub.execute_input":"2025-09-30T06:49:58.305196Z","iopub.status.idle":"2025-09-30T06:49:58.313071Z","shell.execute_reply.started":"2025-09-30T06:49:58.305176Z","shell.execute_reply":"2025-09-30T06:49:58.312297Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:41:20.792967Z","iopub.execute_input":"2025-09-30T07:41:20.793231Z"}},"outputs":[{"name":"stderr","text":"Processing epoch 00: 100%|██████████| 3638/3638 [25:57<00:00,  2.34it/s, loss=4.092]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: In the first place, I was removed from all the wickedness of the world here; I had neither the lusts of the flesh, the lusts of the eye, nor the pride of life.\nTARGET: Primieramente io quivi era lontano da tutte le perversità della terra: non quivi le tentazioni della carne, non le seduzioni dell’occhio, non l’orgoglio della vita.\nPREDICTED: Al primo luogo ove era morto , ero stata per tutti i miei pensieri ; non avevo altro che non avevo altro che mi aveva l ' orgoglio della vita .\n--------------------------------------------------------------------------------\nSOURCE: I also ordered him to bury the horrid remains of their barbarous feast, which I could not think of doing myself; nay, I could not bear to see them if I went that way; all which he punctually performed, and effaced the very appearance of the savages being there; so that when I went again, I could scarce know where it was, otherwise than by the corner of the wood pointing to the place.\nTARGET: Così pure gl’ingiunsi di sotterrare gli orridi avanzi del barbaro loro banchetto, cosa che non avrei avuto stomaco di far io, e da vero se fossi andato colà mi sarebbe mancato il coraggio sin di guardarli. Ma Venerdì eseguì sì puntualmente i miei comandi, che quando tornai colà, non avrei quasi ravvisato più il sito, se non me lo avesse indicato quella punta di bosco donde si cominciò a far fuoco.\nPREDICTED: a prendere il mio cannocchiale , che non mi diedi a , che non mi diedi a pensare a me ; ma non mi diedi a vedere se non avessi potuto le quali fossero stati , e che mi , e che , se avessi potuto , e che , se non fossero stati , e che , se ne fossero stati .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 01: 100%|██████████| 3638/3638 [26:01<00:00,  2.33it/s, loss=4.075]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: Karenin's success was even greater than he had expected.\nTARGET: Il successo di Aleksej Aleksandrovic fu persino maggiore di quello che egli si aspettava.\nPREDICTED: Il successo di Aleksej Aleksandrovic era ancora maggiore di quello che aveva immaginato .\n--------------------------------------------------------------------------------\nSOURCE: The cry died, and was not renewed.\nTARGET: Ma il grido non si fece più udire.\nPREDICTED: Il grido era assai , ed era non di nuovo .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 02: 100%|██████████| 3638/3638 [26:03<00:00,  2.33it/s, loss=3.468]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: She went back to her corner and sat down.\nTARGET: Ella tornò nel suo angolo e sedette.\nPREDICTED: Ella si diresse in un ’ occhiata a lei .\n--------------------------------------------------------------------------------\nSOURCE: It was where the tow-path shelves gently down into the water, and we were camping on the opposite bank, noticing things in general.\nTARGET: Fu dove la strada d’alzaia discende pianamente nell’acqua, e noi stavamo accampati sulla riva opposta, osservando le cose in generale.\nPREDICTED: Si trovava di il pozzo così , si vestì da parte all ’ estremità dell ’ accampamento , e , alla riva sembrava generale , alla riva ci si trovava in generale .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 03: 100%|██████████| 3638/3638 [26:04<00:00,  2.33it/s, loss=3.555]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\nSOURCE: Serezha sank down on his bed and began to sob, hiding his face in his hands.\nTARGET: Serëza si lasciò andare giù sul letto e si mise a singhiozzare, coprendosi il viso con le mani.\nPREDICTED: Serëza si fece pensieroso e cominciò a leggere le mani sul viso di Levin .\n--------------------------------------------------------------------------------\nSOURCE: And the leaves of the trees that grew in the wood were very dark and thick, so that no ray of light came through the branches to lighten the gloom and sadness.\nTARGET: E le foglie degli alberi che crescevano nella foresta erano oscurissime e folte tanto che non un raggio di luce filtrava a traverso i rami ad attenuare la tenebra e la tristezza.\nPREDICTED: E le foglie si i bosco che in quel bosco non facevano notare le impressioni che si coprivano dei , anche per la singolare tristezza e tristezza .\n","output_type":"stream"},{"name":"stderr","text":"Processing epoch 04:   1%|          | 40/3638 [00:17<25:48,  2.32it/s, loss=2.142]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}